```{r setup, include=FALSE, warning=FALSE,message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(janitor)
library(skimr)
library(ggplot2)
library(dplyr)
library(lubridate)
library(RSocrata)
library(tidyquant)
library(readr)
library(readxl)
library(tidytext)
library(janitor)
library(devtools)
library(Rcpp)
library(topicmodels)
```

```{r, message=FALSE, warning=FALSE}
#devtools::install_github("gaospecial/wordcloud2")
```

```{r, message=FALSE, warning=FALSE}
#devtools::install_github("gaospecial/wordcloud2", force = TRUE)
library(wordcloud2)
```

# Setup 

-------- 
```{r, message=FALSE, warning=FALSE}
MSBA20JobDescriptions <- read_excel("MSBA20JobDescriptions.xlsx") %>%
  clean_names()
head(MSBA20JobDescriptions, 5)
```

##term_frequency table for wordcloud. 

#0. make a vector called excludes <- this is just an exmaple c("key", "clients", "chicago")
#1. pipe jobs into unnest_tokens(word, job_description)
#2. pipe into anti_join(stop_words, by = c("word" = "word")) this will remove common words
#3. pipe into filter(!word %in% excludes ) this will remove excluded words  
#  - remove "key", "clients", "chicago" and any other words you think make sense 
#4. pipe into filter(!str_detect(word,"^\\d")) this will remove digits 
#5. group_by(word) and summarize or use count()
#6. arrange(desc(n)) 
#7. term_frequency table 
#8. print out the the top 20 terms using top_n() or slice_max()

```{r, message=FALSE, warning=FALSE}
excludes <- c("accounting", "developer", "programmer", "C", "of", "the", "our", "clients", "are", "a", "years", "of", "on", "for", "hardware", "with", "to", "an", "which", "was", "and", "do", "may", "is", "from", "then", "as", "in", "will", "be", "there")
term_freq_job_descriptions <- MSBA20JobDescriptions %>%
  unnest_tokens(word, job_description) %>%
  anti_join(stop_words, by = c("word"="word")) %>%
  filter(!word %in% excludes) %>%
  filter(!str_detect(word,"^\\d")) %>%
  group_by(word) %>%
  summarise(n = n()) %>%
  arrange(desc(n))
term_freq_job_descriptions %>%
  slice_max(n, n = 20)
```

```{r}
term_freq_firm <- MSBA20JobDescriptions %>%
unnest_tokens(word, firm) %>%
  anti_join(stop_words, by = c("word"="word")) %>%
  filter(!word %in% excludes) %>%
  filter(!str_detect(word,"^\\d")) %>%
  group_by(word) %>%
  summarise(n = n()) %>%
  arrange(desc(n))

term_freq_job_titles <- MSBA20JobDescriptions %>%
unnest_tokens(word, title) %>%
  anti_join(stop_words, by = c("word"="word")) %>%
  filter(!word %in% excludes) %>%
  filter(!str_detect(word,"^\\d")) %>%
  group_by(word) %>%
  summarise(n = n()) %>%
  arrange(desc(n))
```

### wordcloud with all of the terms using wordcloud2()

```{r, message=FALSE, warning=FALSE}
term_freq_job_descriptions %>%
  wordcloud2()
```

### wordcloud for the terms that start with the letter "a" 

```{r, message=FALSE, warning=FALSE}
term_freq_job_descriptions_a <- term_freq_job_descriptions %>%
  filter(str_starts(word, "a"))
term_freq_job_descriptions_a %>%
  wordcloud2()

```

### Task 1c. 

Create a word cloud of companies 

```{r, message=FALSE, warning=FALSE}
term_freq_firm %>%
  wordcloud2()
```


### Task 1d. 

Create a word cloud of job titles. 

```{r, message=FALSE, warning=FALSE}
term_freq_job_titles %>%
  wordcloud2()
```

## Words AFTER "data" ... 

#1. pipe jobs 
#2. into: unnest_tokens(bigram, job_description, token = "ngrams", n = 2, n_min = 2)
#3. separate bigram into two words: separate(bigram, c("word1", "word2"), sep = " ")
#4. filter for "data" filter(word1 == "data") 
#5. remove junk words on word2, filter(!word2 %in% excludes ) this will remove excluded word
#6. unite word 1 and 2 together into a bigram
#7. group_by(bigram)
#8. summarize(n=n())
#9. arrange(desc(n))
#10. save data_term_frequency 
#11. print top 10 data + terms 

```{r, message=FALSE, warning=FALSE}
data_term_freq <- MSBA20JobDescriptions %>%
  unnest_tokens(bigram, job_description, token = "ngrams", n = 2, n_min = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(word1 == "data") %>%
  filter(!word2 %in% excludes) %>%
  unite(bigram, word1, word2, sep = " ", remove = TRUE, na.rm = FALSE) %>%
  group_by(bigram) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
data_term_freq %>%
  head(10)

bigram_freq <- MSBA20JobDescriptions %>%
  unnest_tokens(bigram, job_description, token = "ngrams", n = 2, n_min = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% excludes) %>%
  filter(!word2 %in% excludes) %>%
  unite(bigram, word1, word2, sep = " ", remove = TRUE, na.rm = FALSE) %>%
  group_by(bigram) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
bigram_freq %>%
  head()
```

## wordcloud of "data" + term combinations

```{r, message=FALSE, warning=FALSE}
data_term_freq %>%
  wordcloud2()
```

## bar chart of top 15, "data" + term combinations

```{r, message=FALSE, warning=FALSE}
data_term_freq %>%
  head(15) %>%
  ggplot(aes(x = reorder(bigram, -n), y = n)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Frequency of Top 15 'Data'-Related Bigrams", x = "Bigrams", y = "Frequency")
```

# Technology Term Analysis    

```{r, message=FALSE, warning=FALSE}
technology_words <- c(
    "analytics", 
    "data",
    "analyze",
    "r", 
    "python", 
    "sql", 
    "excel", 
    "cloud",
    "aws",
    "azure",
    "ec2",
    "sas",
    "spss",
    "saas",
    "spark",
    "tensorflow",
    "sagemaker",
    "tableau",
    "hadoop",
    "pyspark",
    "h2o.ai",
    "spark", 
    "ai",
    "shiny",
    "dash",
    "pca",
    "k-means",
    "emr",
    "mapreduce",
    "nosql",
    "hive"
    )

technology_bigram <- c(
  "amazon web",
  "big data",
  "business analytics",
  "google cloud",
  "microsoft azure",
  "machine learning",
  "data science",
  "deep learning",
  "neural network",
  "neural networks",
  "neural nets",
  "random forests",
  "random forest",
  "elastic search",
  "map reduce",
  "artificial intelligence"
)
```


## Create Word & Bi-Gram Frequencies - Jobs 
#1. Filter term_frequency table based on the technology_terms provided, tech_term_freq
#2. Filter bigram_frequency table based on technology_bigrams provided, tech_bigram_freq
#3. Combine results together into technology_term_frequency using using bind_rows()

```{r, message=FALSE, warning=FALSE}
tech_term_freq <- term_freq_job_descriptions %>%
  filter(word %in% technology_words)
tech_bigram_freq <- bigram_freq %>%
  filter(bigram %in% technology_bigram)
technology_terms_bind <- bind_rows(tech_term_freq, tech_bigram_freq)
technology_terms_bind
```

### Bar Chart of Technolgy Terms 

```{r, message=FALSE, warning=FALSE}
tech_term_freq %>%
  ggplot(aes(x = reorder(word, -n), y = n)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Most Frequently Used Technology Terms", x = "Terms", y = "Number of Appearances")
```

### Wordcloud of Techology Terms 

```{r, message=FALSE, warning=FALSE}
tech_term_freq %>%
  wordcloud2()
```


## Try with Your Resume 
#1. save/print your resume as PDF, it needs to be PDF format for this to work.  
#2. install pdftools and load the library(pdftools)
#3. use pdf_text to read your resume into a table. for example here's how i read my resume:  
#   - note: this makes a character vector of resume 
  
```{r, message=FALSE, warning=FALSE}
library(pdftools)
resume <- pdf_text("resume.pdf")
```
  
## Parse resume, filter out common words, digits, and any exclusions 
#Perform following on char vector resume: 
#1. as_tibble(resume) will convert it to a data frame 
#2. parse text into words, using unnest_tokens(word, value)
#3. remove words you want to exclude 
#4. remove numbers
#5. group by words and count the words up. to make a resume_word_freq table.  

```{r, message=FALSE, warning=FALSE}
resume_word_freq <- pdf_text("resume.pdf") %>%
  as_tibble(resume) %>%
  unnest_tokens(word, value) %>%
  anti_join(stop_words, by = c("word"="word")) %>%
  filter(!word %in% excludes,
         !str_detect(word, "\\d"),
         !str_detect(word, ".edu"),
         !str_detect(word, "gpa"),
         !str_detect(word, "age"),
         !str_detect(word, "school"),
         !str_detect(word, "college"),
         !str_detect(word, "learned"),
         !str_detect(word, "north"),
         !str_detect(word, "agreements"),
         !str_detect(word, "access"),
         !str_detect(word, "site"),
         !str_detect(word, "based"),
         !str_detect(word, "actions"),
         !str_detect(word, "assess"),
         !str_detect(word, "arm"),
         !str_detect(word, "arts"),
         !str_detect(word, "ide"),
         !str_detect(word, "hrs"),
         !str_detect(word, "camp"),
         !str_detect(word, "aided"),
         !str_detect(word, "fall"),
         !str_detect(word, "events"),
         !str_detect(word, "day"),
         !str_detect(word, "april"),
         !str_detect(word, "may"),
         !str_detect(word, "june"),
         !str_detect(word, "july"),
         !str_detect(word, "august"),
         !str_detect(word, "time"),
         !str_detect(word, "list"),
         !str_detect(word, "status"),
         !str_detect(word, "basis"),
         !str_detect(word, "participated"),
         !str_detect(word, "additional"),
         !str_detect(word, "aided"),
         !str_detect(word, "public"),
         !str_detect(word, "location"),
         !str_detect(word, ".com"),
         !str_detect(word, "activities"),
         !str_detect(word, "arranged"),
         !str_detect(word, "staff"),
         !str_detect(word, "attended"),
         !str_detect(word, "dean"),
         !str_detect(word, "legal"),
         !str_detect(word, "biweekly"),
         !str_detect(word, "center"),
         !str_detect(word, "communicated"),
         !str_detect(word, "relevant"),
         !str_detect(word, "coursework"),
         !str_detect(word, "created"),
         !str_detect(word, "create"),
         !str_detect(word, "files"),
         !str_detect(word, "multiple"),
         !str_detect(word, "psi"),
         !str_detect(word, "designed"),
         !str_detect(word, "instructed"),
         !str_detect(word, "intern"),
         !str_detect(word, "head"),
         !str_detect(word, "index"),
         !str_detect(word, "athletes"),
         !str_detect(word, "court"),
         !str_detect(word, "base"),
         !str_detect(word, "current"),
         !str_detect(word, "term"),
         !str_detect(word, "accurately"),
         !str_detect(word, "e.g"),
         !str_detect(word, "form"),
         !str_detect(word, "ocean"),
         !str_detect(word, "alternatives"),
         !str_detect(word, "safety"),
         !str_detect(word, "iii"),
         !str_detect(word, "mac"),
         !str_detect(word, "sound"),
         !str_detect(word, "house"),
         !str_detect(word, "variety"),
         !str_detect(word, "plans"),
         !str_detect(word, "conducted"),
         !str_detect(word, "county"),
         !str_detect(word, "march"),
         !str_detect(word, "issues"),
         !str_detect(word, "tasks"),
         !str_detect(word, "perry"),
         !str_detect(word, "detect"),
         !str_detect(word, "week"),
         !str_detect(word, "accustom"),
         !str_detect(word, "skill"),
         !str_detect(word, "range"),
         !str_detect(word, "meetings"),
         !str_detect(word, "skills")) %>%
  group_by(word) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
resume_word_freq
```

## Wordcloud of the remaining words in resume
```{r, message=FALSE, warning=FALSE}
resume_word_freq %>%
  wordcloud2()
```

### Filter for technology_words and technology_bigrams 
#need to Bigram Freq. of resume and filter for technology_bigrams   
#1. as_tibble(resume) pipe into 
#2. unnest_tokens(word, value, token = "ngrams", n = 2, n_min = 2) to make bigrams 
#3. filter(word %in% technology_bigram ) for specific bigrams 
#4. group_by(word)
#5. summarize(n=n()) 
#6.  arrange(desc(n)) 
#7.  make a resume_bigram_freq

```{r, message=FALSE, warning=FALSE}
resume_bigram_freq <- pdf_text("Tyler_Grosman_Resume.pdf") %>%
  as_tibble(resume) %>%
  unnest_tokens(word, value, token = "ngrams", n = 2, n_min = 2) %>%
  filter(word %in% technology_bigram) %>%
  group_by(word) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
resume_bigram_freq
```

###Combine resume_word_freq & resume_bigram_freq together  
```{r, message=FALSE, warning=FALSE}
resume_word_bigram_freq <- bind_rows(resume_word_freq, resume_bigram_freq)
resume_word_bigram_freq
```

### Bar chart of combined technolgy term frequencies
```{r, message=FALSE, warning=FALSE}
resume_word_bigram_freq %>%
  head(50) %>%
  ggplot(aes(x = reorder(word, -n), y = n)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Frequency of Tech Words and Bigrams within My Resume", x = "Tech Words and Bigrams", y = "Frequency")
```


### Wordcloud of combined technolgy term frequencies
```{r, message=FALSE, warning=FALSE}
resume_word_bigram_freq %>%
  wordcloud2()
```

### Compare job descriptions 

#Compare resume's terms to terms found in job descriptions 
```{r, message=FALSE, warning=FALSE}
resume_word_bigram_freq %>%
  filter(word %in% term_freq_job_descriptions$word)
```

#Table of terms missing from resume but found in job descriptions. 
```{r, message=FALSE, warning=FALSE}
tech_term_freq %>%
  filter(!word %in% resume_word_bigram_freq$word)
```

#Tech-skills resume has in common with jobs
```{r, message=FALSE, warning=FALSE}
tech_term_freq %>%
  filter(word %in% resume_word_bigram_freq$word)
```

#Skills are missing from resume
```{r, message=FALSE, warning=FALSE}
tech_term_freq %>%
  filter(!word %in% resume_word_bigram_freq$word)
```
