```{r setup, include=FALSE, warning=FALSE,message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Libraries
```{r, message= FALSE, warning=FALSE}
library(topicmodels)
library(readr)
library(tidyverse)
library(lubridate)
library(dplyr)
library(tidytext)
library(rtweet)
library(readxl)
library(janitor)
library(devtools)
library(Rcpp)
library(htmlwidgets)
library(wordcloud2)
```
###Analyzing Trump's tweets: are they harfmul or innocuous? let's find out...
### load tweets 

Note the data is pipe delimited( delim = "|") so need to read them with read_delim instead of read_csv; might also need to transform created_at as a date variable (col_types = cols(created_at = col_datetime(format = "%m-%d-%Y %H:%M:%S"))) 

```{r, message=FALSE, warning=FALSE}
tweets <- read_delim("TrumpQ12020Tweets.csv", delim = "|", col_types = cols(created_at = col_datetime(format = "%m-%d-%Y %H:%M:%S")))
```

## Term Frequency & Wordcloud 

## Create tweet_freq table
#1. create a month_varaible 
#2. parse terms into words, remove the following 
#  - stop words
#  - c("t.co", "https", "false", "twitter", "iphone", "amp", "rt", "android")
#3. summarize by month and word
#4. take top 100 words by month 

## Wordclouds:
#1. word cloud of all terms 
#2. word cloud for month 1 
#3. word cloud for month 2 
#4. word cloud for month 3 

```{r, message=FALSE, warning=FALSE}
tweet_frequency <- tweets %>%
  mutate(month = paste(month(created_at, label = TRUE, abbr = TRUE))) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  filter(!word %in% c("t.co", "https", "false", "twitter", "iphone", "amp", "rt", "android")) %>%
  filter(!str_detect(word, "^\\d")) %>%
  group_by(word, month) %>%
  summarize(n = n()) %>%
  ungroup() %>%
  arrange(desc(n))
tweet_frequency %>%
  head(100)
```

```{r, message=FALSE, warning=FALSE}
tweet_frequency %>%
  filter(month == "Jan") %>%
  select(word = word, n = n) %>%
  wordcloud2()
```

```{r, message=FALSE, warning=FALSE}
tweet_frequency %>%
  filter(month == "Feb") %>%
  select(word = word, n = n) %>%
  wordcloud2()
```

```{r, message=FALSE, warning=FALSE}
tweet_frequency %>%
  filter(month == "Mar") %>%
  select(word = word, n = n) %>%
  wordcloud2()
```

# The @realdonaldtrump handle as well as the words, "democrat", "president", "impeachment", "whitehouse", "coronavirus", "news", "senate", "house", "american", and "people" are all mentioned very prominently throughout these three months. Additionally, Trump's name is mentioned often within his own tweets, but this is likely due to him reposting articles or pieces on his Twitter page that reference or include his own name. 

## Bigram Analysis 

## Bigram_freq table: 
#1. create a bigram 
#2. summarize by bigram 
#3. use separate to split bigram into word1 and word2 then filter the following
#  - stop words against both word1 and word2 
#  - c("t.co", "https", "false", "twitter", "iphone", "amp", "rt", "android")
#  - filter digits 
#4. create a bigram varaible by combining word1 and word2 together 

## Explore
#1. wordcloud of top 100 bigram terms. 
#2. chart of the top 10 terms that come after the word "fake", be sure to use coordinate flip 
#3. chart of the top 10 terms that come before the word "media", be sure to use coordinate flip 
#4. chart of the top 3 terms that before "joe", be sure to use coordinate flip 

```{r, message=FALSE, warning=FALSE}
bigram_frequency <- tweets %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2, collapse = NULL) %>%
  group_by(bigram) %>%
  summarize(n = n()) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  anti_join(stop_words, by = c("word1" = "word")) %>%
  anti_join(stop_words, by = c("word2" = "word")) %>%
  filter(!word1 %in% c("t.co", "https", "false", "twitter", "iphone", "amp", "rt", "android")) %>%
  filter(!str_detect(word1, "^\\d")) %>%
  filter(!word2 %in% c("t.co", "https", "false", "twitter", "iphone", "amp", "rt", "android")) %>%
  filter(!str_detect(word2, "^\\d")) %>%
  mutate(bigram = str_c(word1, word2, sep = " ")) %>%
  arrange(desc(n))
bigram_frequency
```

```{r, message=FALSE, warning=FALSE}
bigram_frequency %>%
  top_n(100, n) %>%
  select(word = bigram, n = n) %>%
  wordcloud2()
```

```{r, message=FALSE, warning=FALSE}
bigram_frequency %>%
  filter(word1 == "fake") %>%
  filter(!str_detect(word2, "\\d")) %>%
  top_n(10, n) %>%
  ggplot(aes(x = reorder(bigram, n), y = n, fill = NULL)) +
  geom_col(show.legend = FALSE) +
  labs(title = "Top 10 Terms Containing 'Fake' as Word1", x = NULL, y = "Frequency") +
  coord_flip()
```

```{r, message=FALSE, warning=FALSE}
bigram_frequency %>%
  filter(word2 == "media") %>%
  filter(!str_detect(word1, "\\d")) %>%
  top_n(10, n) %>%
  ggplot(aes(x = reorder(bigram, n), y = n, fill = NULL)) +
  geom_col(show.legend = FALSE) +
  labs(title = "Top 10 Terms Containing 'Media' as Word2", x = NULL, y = "Frequency") +
  coord_flip()
```

```{r, message=FALSE, warning=FALSE}
bigram_frequency %>%
  filter(word2 == "joe",
         !str_detect(word1, "\\d")) %>%
  top_n(3, n) %>%
  ggplot(aes(x = reorder(bigram, n), y = n, fill = NULL)) +
  geom_col(show.legend = FALSE) +
  labs(title = "Top 10 Terms Containing 'Joe' as Word2", x = NULL, y = "Frequency") +
  coord_flip()
```

# tons of nickname/colloquial nomenclature present (e.g., "mini mike" and "impeachment hoax"); "president" followed by Trump's Twitter handle was most frequently occurring bigram; must be due to the fact that there lies a purpose in tagging oneself when posting a tweet (I have no idea how twitter works....)

### Sentiment analysis

## sentiment_by_month 
#1. inner join words_by_month to "bing" sentiments 
#2. group by month and sentiment 
#3. get the top 10 words by month 
#4. make words with negative sentiment negative (-n) and positive words positive

##bar charts 
#chart 1 different sentiment for months 1/2/3, order n, coord_flip

```{r, message=FALSE, warning=FALSE}
sentiment_by_month <- tweet_frequency %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  group_by(month, sentiment) %>%
  top_n(10, month) %>%
  mutate(sentiment_score = if_else(sentiment == "negative", -n, n))
sentiment_by_month
```

```{r, message=FALSE, warning=FALSE}
sentiment_by_month %>%
  filter(month == "Jan") %>%
  top_n(10, n) %>%
  ggplot(aes(x = reorder(word, sentiment_score), y = sentiment_score, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  labs(title = "Sentiments for the Month of January", x = "Word", y = "Frequency") +
  coord_flip()
```

```{r, message=FALSE, warning=FALSE}
sentiment_by_month %>%
  filter(month == "Feb") %>%
  top_n(10, n) %>%
  ggplot(aes(x = reorder(word, sentiment_score), y = sentiment_score, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  labs(title = "Sentiments for the Month of February", x = "Word", y = "Frequency") +
  coord_flip()
```

```{r, message=FALSE, warning=FALSE}
sentiment_by_month %>%
  filter(month == "Mar") %>%
  top_n(10, n) %>%
  ggplot(aes(x = reorder(word, sentiment_score), y = sentiment_score, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  labs(title = "Sentiments for the Month of March", x = "Word", y = "Frequency") +
  coord_flip()
```

# Certain categorizations of words that were not previously defined; was able to define them by unnesting the text within the tweets (and words within the text) while providing sentiment values to certain words; able to create positive and negative categorizations of words or phrases, which we could then categorize as positive/negative groups or typologies of words

## Topic Prep 
#preparing a Document Term Matrix (dtm) - tweet_dtm
#1. unest tokens into words 
#2. remove the following 
#  - stop words
#  - c("t.co", "https", "false", "twitter", "iphone", "amp", "rt", "android")
#3. summarize by id_str (tweet id) and word
#4. take top 20 words by id 

create tweet_lda by taking your tweet_dtm, pick a value of k (4,6,8 or 10)

```{r, message=FALSE, warning=FALSE}
tweet_dtm <- tweets %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  filter(!word %in% c("t.co", "https", "false", "twitter", "iphone", "amp", "rt", "android")) %>%
  filter(!str_detect(word, "\\d")) %>%
  group_by(id_str, word) %>%
  summarize(n = n()) %>%
  top_n(20, id_str) %>%
  cast_dtm(id_str, word, n)  
tweet_lda <- LDA(tweet_dtm, k = 4, control = list(seed = 1000))
```

## Topic Model 
#1. document term matrix needs to be cleaned up and generate beta 
#2. generate topic terms by extracting top_n by beta 
#3. plot your topics 

```{r, message=FALSE, warning=FALSE}
tidy_tweet_lda <- tidy(tweet_lda, matrix = "beta")
topic_tweet_terms <- tidy_tweet_lda %>%
  group_by(topic) %>%
  top_n(8, beta) %>%
  arrange(topic) %>%
  filter(!term %in% c("realdonaldtrump", "people", "time", "president", "trump", "mini")) %>%
  mutate(term = reorder(term, beta)) %>%
  ungroup() %>%
  arrange(desc(beta)) %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(title = "Top 4 Topic Categories", x = NULL, y = expression(beta)) +
  facet_wrap(~ topic, ncol = 2, scales = "free") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1.5, hjust = 1.5))
topic_tweet_terms
```

#Identifiable Topics of Trump's Tweets:
#Topic I - Bernie Sanders
#Topic II - Impeachment & COVID-19
#Topic III - Democrats & the House of Representatives
#Topic IV - Democrats, Fake News, and the Senate
